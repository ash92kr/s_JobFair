### 12강 의사결정나무



* 의사결정나무

기계학습 중 하나로 의사결정 규칙을 나무 형태로 분류하는 분석 기법

```
분석 과정이 직관적이고 이해하기 쉬움 -> 가장 중요한 기준부터 위쪽에 위치함
연속형/범주형 변수 모두 사용 가능
분지규칙은 불순도(범주들이 섞여있는 정도)를 최소화시키는 곳에서 선정됨
```



1) tree 형성(growing tree) -> 과적합 우려

2) tree 가지치기(pruning tree) 

3) 최적 tree로 분류(classification)



```
library(tree)   # 의사결정나무 수행을 위한 패키지
library(caret)  # 오분류율 교차표 생성을 위한 패키지

iris<-read.csv("iris.csv")

set.seed(1000)
N<-nrow(iris)
tr.idx<-sample(1:N, size=N*2/3, replace=FALSE)   # 데이터 분할(학습데이터 2/3, 검증데이터 1/3)

train<-iris[tr.idx,]
test<-iris[-tr.idx,]

treemod<-tree(Species~., data=train)   # tree(종속변수~독립변수, data= )
treemod   # 분지결과 저장
plot(treemod)   # 의사결정나무 분지를 그림으로 표현




```



```
node), split, n, deviance, yval, (yprob)
      * denotes terminal node

 1) root 100 219.500 versicolor ( 0.34000 0.35000 0.31000 )  
   2) Petal.Length < 2.45 34   0.000 setosa ( 1.00000 0.00000 0.00000 ) *
   # petal.Length가 2.45보다 작은 34개는 setosa로 분류하고 종료(*)
   3) Petal.Length > 2.45 66  91.250 versicolor ( 0.00000 0.53030 0.46970 )
   # petal.Length가 2.45보다 큰 66개는 versicolor로 분류하고 하위 가지에서 계속 분리
     6) Petal.Width < 1.75 37  20.820 versicolor ( 0.00000 0.91892 0.08108 )  
      12) Petal.Width < 1.45 26   0.000 versicolor ( 0.00000 1.00000 0.00000 ) *
      13) Petal.Width > 1.45 11  12.890 versicolor ( 0.00000 0.72727 0.27273 ) *
     7) Petal.Width > 1.75 29   8.700 virginica ( 0.00000 0.03448 0.96552 )  
      14) Petal.Length < 4.95 5   5.004 virginica ( 0.00000 0.20000 0.80000 ) *
      15) Petal.Length > 4.95 24   0.000 virginica ( 0.00000 0.00000 1.00000 ) *
```



![의사결정나무](https://user-images.githubusercontent.com/43332543/58615136-4b186880-82f5-11e9-93d0-7e0a51e87462.png)



```
# 최적 tree모형을 위한 가지치기
cv.trees<-cv.tree(treemod, FUN=prune.misclass)   # prune.misclass함수를 사용해 가지치기
cv.trees
plot(cv.trees)
```



```
$size
[1] 5 3 2 1   # k=0일 때의 최적터미널 노드 수는 3

$dev
[1]  5  5 45 78

$k   # 복잡도계수(k)의 값이 최소가 되는 노드수 선택
[1] -Inf    0   27   34

$method
[1] "misclass"

attr(,"class")
[1] "prune"         "tree.sequence"
```



cv.tree함수에서 가지치기함수는 prune.tree 혹은 prun.misclass를 사용할 수 잇다



```
prune.trees<-prune.misclass(treemod, best=3)   # 최종 터미널 노드를 3으로 지정
plot(prune.trees)
text(prune.trees,pretty=0, cex=1.5)
```



![의사결정나무_최종](https://user-images.githubusercontent.com/43332543/58615638-89625780-82f6-11e9-9fad-6c46d13fe0db.png)



```
# 위 최종 의사결정나무 모형에 검증 데이터를 넣고 정확도 구함
treepred<-predict(prune.trees, test, type='class')
confusionMatrix(treepred,test$Species)
```



* rpart, party 패키지를 통한 의사결정나무 만들기



```
library(rpart)
library(party)
library(caret)

iris<-read.csv("iris.csv")

# ...데이터 분할 및 샘플링...

cl1<-rpart(Species~., data=train)   # 종속변수~독립변수, data
plot(cl1)
text(cl1, cex=1.5)
```



* rpart 함수는 tree 패키지에서 가지치기(pruning)를 한 결과와 동일하지만, 데이터에 따라 부가적인 가지치기가 필요할 수도 있다



```
# rpart 패키지 가지치기
printcp(cl1)
plotcp(cl1)

pcl1<-prune(cl1, cp=cl1$cptable[which.min(cl1$cptable[,"xerror"]),"CP"])
# xerror가 최소의 값인 것의 CP를 구함
plot(pcl1)
text(pcl1)
```



```
Classification tree:
rpart(formula = Species ~ ., data = train)

Variables actually used in tree construction:
[1] Petal.Length Petal.Width 

Root node error: 65/100 = 0.65

n= 100 
# xerror(cross validation error)의 값이 최소가 되는 마디를 선택
       CP nsplit rel error   xerror     xstd   
1 0.52308      0  1.000000 1.153846 0.066617
2 0.41538      1  0.476923 0.476923 0.071153
3 0.01000      2  0.061538 0.076923 0.033530
```



```
# 의사결정나무 결과 test data에 대한 정확도
pred2<- predict(cl1,test, type='class')
confusionMatrix(pred2, test$Species)
```



```
# party 패키지를 통한 의사결정나무 만들기
partymod<-ctree(Species~., data=train)
plot(partymod)
```



![ctree](https://user-images.githubusercontent.com/43332543/58616213-10fc9600-82f8-11e9-976a-5d698a60de8f.png)

* p-value를 기준으로 해서 분지기준을 잡음

Petal.Width가 0.6 이하인 34개는 모두 setosa로 분류

남은 66개는 기준에 따라 versicolor와 virginica로 분류

우측의 3개는 2가지의 분류가 섞여 있으므로 불순도가 0보다 높다



```
# 검증데이터 넣고 정확률 구함
partypred<-predict(partymod,test)
confusionMatrix(partypred,test$Species)
```



* 랜덤포레스트

Leo Breiman이 2001년 만든 기법으로 의사결정나무의 단점(과적합)을 개선한 알고리즘

앙상블(Ensemble) 기법을 사용한 모델로서 주어진 데이터로 리샘플링을 통해 다수의 의사결정나무를 만든 다음, 여러 모델의 예측 결과들을 종합해 정확도를 높이는 방법

```
training data로부터 표본의 크기가 n인 bootstrap sample(랜덤 리샘플링) 추출
tree모형 구형(1 ~ k)
각 모델 tree들의 앙상블 결과를 출력
```



(1) Bagging(Bootstrap Aggregating)

전체 데이터에서 학습데이터를 복원추출(resampling)해 트리를 구성 = Training Data에서 random sampling

학습 데이터에서 k개의 데이터셋을 리샘플링해 구성 -> 각 데이터셋마다 의사결정나무 만듦 -> 최종 모델 선정



```
library(randomForest)
library(caret)

iris<-read.csv("iris.csv")

# ... 데이터 분할 및 샘플링 ...
# ntree : 의사결정나무를 몇 단계까지 구성할 것인가, sampsize : 샘플링 사이즈, importance : 변수 중요도 표시 여부

rf_out1<-randomForest(Species~., data=train, importance=T)
# mtry 변수 : n개의 변수 중 몇 개의 변수만 뽑아서 의사결정나무를 만들지 결정하는 옵션
# default = sqrt(p)로 만듦
rf_out1
```



```
Call:
 randomForest(formula = Species ~ ., data = train, importance = T) 
               Type of random forest: classification
                     Number of trees: 500   # tree 개수 500개
No. of variables tried at each split: 2   # 사용 변수 개수는 2개

        OOB estimate of  error rate: 6%   # 오분류율
Confusion matrix:
           setosa versicolor virginica class.error
setosa         34          0         0  0.00000000
versicolor      0         32         3  0.08571429
virginica       0          3        28  0.09677419
```



```
rf_out2<-randomForest(Species~., data=train, importance=T, mtry=4)
# mtry가 높을 때가 일반적으로 정확도가 높음 -> 모든 독립변수를 사용해 의사결정나무를 만듦
rf_out2

round(importance(rf_out2), 2)   # 변수 중요도 나옴(소수점 이하 2번째 자리까지만 표시)
randomForest::importance(rf_out2)
```



```
             setosa versicolor virginica MeanDecreaseAccuracy MeanDecreaseGini
Sepal.Length   0.00       5.35      4.30                 7.28             1.27
Sepal.Width    0.00      -2.59      8.27                 4.75             1.36
Petal.Length  22.75      25.14     25.28                28.08            24.36
Petal.Width   21.07      34.74     42.76                37.85            38.95
                                  # 아래 2변수가 분류의 정확도에 기여도가 높은 변수임
```



```
varImpPlot(rf_out2)   # 변수 중요도 시각화
```



![importance](https://user-images.githubusercontent.com/43332543/58617228-8ec1a100-82fa-11e9-8f62-c88bb11a5db9.png)



```
# 랜덤포레스트 모델의 검증 데이터에 대한 정확도
rfpred<-predict(rf_out2,test)
confusionMatrix(rfpred,test$Species)
```

