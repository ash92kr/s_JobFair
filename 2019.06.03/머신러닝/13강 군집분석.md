### 13강 군집분석



* 군집분석 : 비지도학습의 하나로 주어진 데이터의 속성으로 군집화하는 방법

ex) 계층형 군집 분석, k-means(비계층적 군집분석)



유사한 속성을 가진 객체들을 군집(cluster)으로 묶어주는(나누는) 데이터마이닝 기법

ex) 고객들의 구매패턴을 반영하는 속성들에 관한 데이터가 수집된다고 할 때, 군집분석을 통해 유사한 구매패턴을 보이는 고객들을 군집화하고 집단별 판매전략 도출



(1) 계층적 군집(hierarchical clustering) : 사전에 군집 수 k를 정하지 않고 단계적으로 군집 트리 제공

군집 트리에서 적당한 기준으로 k를 나누는 방법(수평선을 그음)

(2) 비계층적 군집(non-hierarchical clustering) : 사전에 군집 수 k를 정한 후 각 객체를 k개 중 하나의 군집에 배정  



* 유사성척도의 계산

객체 간의 유사성 정도를 정량적으로 나타내기 위한 척도

(1) 거리 척도 : 거리가 가까울수록 유사성이 크고, 거리가 멀수록 유사성이 적어짐

ex) 유클라디안 거리 : x축 거리의 제곱 + y축 거리의 제곱의 제곱근

![유클라디안 거리](https://user-images.githubusercontent.com/43332543/58782442-95615880-8619-11e9-8cec-aabf6af4cde4.PNG)



```R
m1 <- matrix(
  c(150, 50, 130, 55, 80, 80, 100, 85, 95, 91),
  nrow = 5,
  ncol = 2,
  byrow = TRUE)
m1 <- as.data.frame(m1)

D1 <- dist(m1)  # defalut 거리는 유클라디안 거리
D1
```



ex) 민코프스키 거리 : 유클라디안 거리의 일반화된 방법(m=2일 때는 유클라디안 거리와 동일)

![민코프스키 거리](https://user-images.githubusercontent.com/43332543/58782836-84651700-861a-11e9-888c-34f7d01ed16a.PNG)

```R
D2<- dist(m1, method="minkowski", p=3) 
D2
```



(2) 상관계수 척도 : 객체간 상관계수가 클수록 두 객체의 유사성이 커짐

ex) 마할라노비스 거리 : 변수 간 상관관계가 존재할 때 사용

![마할라노비스 거리](https://user-images.githubusercontent.com/43332543/58782884-9fd02200-861a-11e9-9721-019c504c371b.PNG)

```R
m2 <- matrix(
  c(20, 6, 14, 30, 7, 15, 46, 4, 2),
  nrow = 3,
  ncol = 3,
  byrow = TRUE)
m2

cor(m2[1,],m2[2,]) 
cor(m2[1,],m2[3,])   # 객체1과 객체2의 유사성이 객체1과 객체3의 유사성보다 크다
```



* 계층적 군집분석

사전에 군집 수 k를 정하지 않고 단계적으로 군집을 형성한다

유사한 객체들을 군집으로 묶고, 그 군집을 기반으로 유사한 데이터를 새로운 군집으로 묶어가면서 군집을 계층적으로 구성함

ex) 단일연결법, 완전연결법, 평균연결법, 중심연결법



(1) 단일연결법

군집 i와 군집 j의 유사성 척도로 두 군집의 모든 객체 쌍의 거리 중 가장 가까운 거리를 사용 → 객체 쌍의 가장 짧은 거리가 작을수록 두 군집이 더 유사하다고 평가

![단일연결법](https://user-images.githubusercontent.com/43332543/58783318-94c9c180-861b-11e9-8f2f-49b2f7f4224b.PNG)

군집 사이 거리(다양한 거리 사용)가 최소인 두 군집을 묶어 하나의 군집으로 만든다

계속해서 거리가 가까운 두 개 군집(하나의 객체도 하나의 군집으로 여김)을 묶어나간다



(2) 완전연결법 : 두 군집의 모든 객체 쌍의 거리 중 가장 먼 거리를 사용

![완전연결법](https://user-images.githubusercontent.com/43332543/58783394-c5116000-861b-11e9-8106-a7c5fcc3e355.PNG)

(3) 평균연결법 : 두 군집의 모든 객체 쌍의 평균 거리를 사용

![평균연결법](https://user-images.githubusercontent.com/43332543/58783400-c6db2380-861b-11e9-9b7f-df50a68c0c66.PNG)

(4) 중심연결법 : 두 군집의 중심 좌표(위치를 가지고 판단)

![중심연결법](https://user-images.githubusercontent.com/43332543/58783396-c6428d00-861b-11e9-8eb7-184263a5f6c9.PNG)



* 덴드로그램 : 군집 그룹과 유사성 수준을 표시하는 트리 다이어그램, 군집이 어떻게 형성되는지 확인하고 형성된 군집의 유사성 수준 평가

![덴드로그램](https://user-images.githubusercontent.com/43332543/58783639-5ed90d00-861c-11e9-8686-0ba9651d11e6.PNG)



```R
library(lattice)
library(DAAG)   # wages1833 데이터를 가져오기 위한 패키지

# 나이, 남성 근로자 수, 남성 근로자 평균 임금, 여성 근로자 수, 여성 근로자 평균 임금
data("wages1833")

dat1 <- wages1833
dat1 <- na.omit(dat1)  # 결측치 데이터 삭제(전처리)

dist_data <- dist(dat1)   # 유클라디안 거리 사용

hc_a <- hclust(dist_data, method = "complete")   # 계층적 군집분석(hclust(거리계산결과, method="single, complete, average, centroid, ward.D2(군집내 제곱합 이용)"))
plot(hc_a, hang = -1, cex=0.7, main = "complete")   # hang = 라벨을 일정한 위치로 고정
```



![complete](https://user-images.githubusercontent.com/43332543/58783885-09513000-861d-11e9-954f-91ed31db9415.png)
![average](https://user-images.githubusercontent.com/43332543/58783890-09e9c680-861d-11e9-8e9f-d1bc079d4dc0.png)
![ward](https://user-images.githubusercontent.com/43332543/58783892-0b1af380-861d-11e9-9b92-618f79eccc42.png)



* 비계층적 군집분석

사전에 군집 수 k를 정한 후 각 객체를 k개 중 하나의 군집에 배정

ex) k-means 알고리즘, k-medoids 알고리즘(PAM, CLARA)



* k-means 군집분석(비계층적 군집분석 중 가장 널리 사용)

k개 군집의 중심좌표를 고려해 각 객체를 가장 가까운 군집에 배정하는 것을 반복

![k-means](https://user-images.githubusercontent.com/43332543/58784155-baf06100-861d-11e9-9a52-3365e5f38a8e.PNG)



초기 객체는 임의로 선정됨 → 객체별 데이터별 거리를 구한 다음, 거리가 짧은 쪽의 군집에 넣어짐

→ 새로운 군집의 중심 좌표 산출 → 수렴 조건 점검(이전 군집결과와 변화가 없으면 최종 군집해가 만들어짐)



* 군집수 k의 결정

```R
library(DAAG)

dat1 <- wages1833
dat1 <- na.omit(dat1)
head(dat1, n=5)

install.packages("factoextra")
library(factoextra)

# 최적 군집수에 대한 시각화 - silhouette, gap_stat, wss(그룹내합계제곱)으로 산출
# 그래프가 완만해지는 지점을 k의 값으로 추정
fviz_nbclust(dat1, kmeans, method = "wss")   
fviz_nbclust(dat1, kmeans, method = "gap_stat")
```

![k_wss](https://user-images.githubusercontent.com/43332543/58784480-829d5280-861e-11e9-821c-93b8877006fb.png)

k=3일 때를 최적이라고 가정



```R
set.seed(123)
km <- kmeans(dat1, 3, nstart = 25)  # nstart = random set의 수
km

fviz_cluster(km, data = dat1, ellipse.type="convex", repel = TRUE)
# convex 모양으로 구역 표시, repel을 통해 관측치 표기
```

![k-means 결과](https://user-images.githubusercontent.com/43332543/58784606-c85a1b00-861e-11e9-86ff-fc7ded0586f5.png)



```
K-means clustering with 3 clusters of sizes 28, 10, 13  # 군집에 속한 개체 수

Cluster means:   # 군집별 변수별 평균
   age     mnum     mwage      fnum    fwage
1 36.5  43.2500 241.73214  31.21429 107.9643
2 55.5   6.9000 178.99000   0.00000   0.0000
3 16.0 187.2308  96.36154 225.23077  71.0000
```



* k-medoids 군집분석

각 군집의 대표 객체를 고려함 : **군집 내 다른 객체들과의 거리가 최소가 되는 객체**

객체들을 k개의 군집으로 구분하는데, 객체와 속하는 군집의 대표 객체와의 거리 총합을 최소로 하는 방법

(1) PAM 알고리즘 : <u>모든 객체에 대하여</u> 대표 객체가 변했을 때 발생하는 거리 총합의 변화를 계산

→ 데이터 수가 많아질수록 연산량이 크게 증가함

(2) CLARA 알고리즘 : <u>적절한 수의 객체를 샘플링 한 후</u>, PAM 알고리즘을 적용하여 대표 객체 선정

샘플링을 여러 번 한 후 가장 좋은 결과를 택함 = 편향된 샘플링은 잘못된 결과값을 도출할 수 있음



```R
library("cluster")
pam_out <- pam(dat1, 3)
pam_out

table(pam_out$clustering)  # 각 군집에 속한 객체의 개수

fviz_cluster(pam_out, data = dat1, ellipse.type="convex", repel = TRUE)
```



```
Medoids:   # 대표 객체
   ID age mnum mwage fnum fwage
7   7  16  204  83.5  256    72
31 31  40   38 243.5   15   104
45 45  54   12 174.0    0     0
```

![pam](https://user-images.githubusercontent.com/43332543/58785107-d65c6b80-861f-11e9-92a7-2c844b61240c.png)



