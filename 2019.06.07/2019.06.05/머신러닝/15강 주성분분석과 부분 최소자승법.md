### 15강 주성분분석과 부분 최소자승법



* 주성분분석

다변량분석기법 중 하나로, 주성분이라고 불리는 선형조합으로 표현하는 기법

주성분은 공분산(X*X)으로부터 eigenvector와 eigenvalue를 도출해서 계산됨



주성분간의 수직관계

PC1(제1주성분) : 독립변수들의 변동(분산)을 가장 많이 설명하는 부분

PC2(제2주성분) : PC1과 수직인 주성분(첫번째 주성분이 설명하지 못하는 변동에 대해 두번째로 설명하는 성분)

![주성분](https://user-images.githubusercontent.com/43332543/59084060-b9cc7600-8934-11e9-8af8-8c5c7ad1def6.PNG)

X1과 X2를 t1과 t2로 축을 변경함 -> PC1이 가장 많이 설명하고, PC2는 적은 부분을 설명한다



주성분분석의 목적 : 차원 축소 및 예측력 향상



* iris 데이터 실습

```R
# 주성분분석은 추가패키지 필요없음
iris<-read.csv(file="iris.csv")

cor(iris[1:4])   # 독립변수간 높은 상관계수가 관찰될 때 차원축소법 실시
```



```
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000
```



```R
# 주성분분석을 위한 함수 prcomp(독립변수들, center= , scale= )
ir.pca <- prcomp(iris[,1:4], center=T, scale.=T)  # 기본값은 center=T, scale=F
# center=T, scale=T는 변수들의 평균을 빼고, 편차로 나누어 표준화한다는 의미
ir.pca
```



```
Standard deviations (1, .., p=4):
[1] 1.7083611 0.9560494 0.3830886 0.1439265

Rotation (n x k) = (4 x 4):
                    PC1         PC2        PC3        PC4
Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863
Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096
Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492
Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971

# PC1 = 0.5211 * Sepal.Length - 0.2693 * Sepal.Width + 0.5804 * Petal.Length + 0.5649 * Petal.Width(선형 조합을 통해 새로운 변수 PC1을 생성함)
```



```R
summary(ir.pca)
```



```
Importance of components:
                          PC1    PC2     PC3     PC4
Standard deviation     1.7084 0.9560 0.38309 0.14393
Proportion of Variance 0.7296 0.2285 0.03669 0.00518
# PC1은 전체분산의 72.96%를 설명, PC2는 전체분산의 22.85%를 설명
Cumulative Proportion  0.7296 0.9581 0.99482 1.00000
# 누적설명비율을 보면 PC1과 PC2, 2개의 성분으로 전체분산의 95.81%를 설명할 수 있다
```



* 최적 주성분 수 구하기 : scree plot을 그려보고 급격히 떨어지기 전까지의 PC를 선택한다

![scree plot](https://user-images.githubusercontent.com/43332543/59084330-c6050300-8935-11e9-9b4a-57ad0fdb1a77.png)

3rd PC에서 설명력이 급격하게 떨어짐(기울기가 꺾이는 PC3를 `elbow point`라고 부름)

즉, 이 경우는 PC2까지 사용하는 것을 추천한다

![screeplot](https://user-images.githubusercontent.com/43332543/59084390-05335400-8936-11e9-9cd6-a24e059e9b51.png)



* PC계산 = `X_data(n*p) %*% PCA_weight(p*p)` : 원래 데이터 행렬과 주성분 가중치 행렬곱 실시

```R
PRC <- as.matrix(iris[,1:4]) %*% ir.pca$rotation   # PRC는 150*4의 행렬을 나타냄

# PC1 = 0.5211 * Sepal.Length - 0.2693 * Sepal.Width + 0.5804 * Petal.Length + 0.5649 * Petal.Width
# 위의 가중치를 곱해서 각 셀의 값이 나옴
```



```R
iris.pc <- cbind(as.data.frame(PRC), Species)

m1<- svm(Species ~., data = iris.pc, kernel="linear")  # SVM을 통한 분류 실행
# PC1 ~ PC4 모두를 input으로 분류모형 수행

# 오분류표 만들기
x<-iris.pc[, -5]
pred <- predict(m1, x)

y<-iris.pc[,5]
table(pred, y)
```



```
# 주성분을 이용한 분류 오분류율이 단순 SVM 오분류율보다 낮은 경우존재
			y
pred         setosa versicolor virginica
  setosa         50          0         0
  versicolor      0         48         0
  virginica       0          2        50
```





* 주성분 회귀분석

독립변수들의 차원을 줄이기 위해 사용하며, 주성분을 이용해 타겟변수의 설명력(예측력)을 높일 수 있다

```
Y = b0 + b1*PC1 + b2*PC2
```

독립변수들의 전체분산을 가장 잘 설명해주는 component를 사용해 독립변수들 간의 다중공선성 문제를 해결할 수 있다



주요 component score가 Y의 예측력을 보장하는 것은 아니다.

주요 component score는 X의 분산을 가장 잘 설명하는 방향의 축을 기준으로 변환된 것이므로 Y와의 관계에 있어서는 상관성이 없을 수도 있다.



* wine 데이터를 통한 실습(타겟변수는 Aroma rating, 9개의 독립변수)

```R
wine<-read.csv(file="wine_aroma.csv")

cor(wine[1:9])   # 독립변수간 상관관계 확인(0.95, 0.82 등 높은 상관계수 존재)

# 주성분 분석
wi.pca<-prcomp(wine[1:9], center=T, scale.=F)   # mean-centering만 하고 component를 뽑음
```



```
Rotation (n x k) = (9 x 9):   # 각 주성분의 변수별 weight를 보여줌
             PC1           PC2           PC3           PC4
Mo  4.396532e-05  1.531130e-03 -0.0016925110  6.688447e-05
Ba -2.644028e-04  1.588626e-03  0.0016225184  2.310870e-02
Cr -4.594881e-05  3.640528e-05  0.0002075367  5.172124e-04
Sr -1.386527e-03  7.314677e-03  0.0074873569  1.258356e-01
Pb  2.621301e-04  6.962105e-03 -0.0076524788  2.814073e-02
B  -2.516393e-03 -1.071784e-02  0.0005113384 -9.913097e-01
Mg -7.437057e-02  9.302614e-01  0.3589981755 -1.087874e-02
Ca -4.369679e-02  3.565893e-01 -0.9331669873 -3.859202e-03
K  -9.962686e-01 -8.506499e-02  0.0141159901  3.311344e-03
```



```
Importance of components:
                            PC1      PC2      PC3     PC4   PC5
Standard deviation     191.3418 27.33235 15.40582 1.57383 0.419
Proportion of Variance   0.9738  0.01987  0.00631 0.00007 0.000
Cumulative Proportion    0.9738  0.99362  0.99993 0.99999 1.000
                          PC6     PC7     PC8    PC9
Standard deviation     0.1704 0.05942 0.02365 0.0109
Proportion of Variance 0.0000 0.00000 0.00000 0.0000
Cumulative Proportion  1.0000 1.00000 1.00000 1.0000

# PC1은 전체분산의 97.38%를 설명 = 즉, 1개의 변수만으로도 독립변수 전체분산을 거의 설명
# 누적설명비율을 보면, PC1과 PC2, 2개의 성분으로 전체분산의 99.36%를 설명
```



![wine_s](https://user-images.githubusercontent.com/43332543/59085335-3bbe9e00-8939-11e9-9630-2ba45b9aec36.png)

2rd PC에서 설명력이 급격하게 떨어짐 = PC1 1개만 사용해도 된다(scree plot이 급격히 떨어지기 전까지의 PC를 선택)



```R
# 원래 데이터 * 주성분 가중치 행렬
PRC<-as.matrix(wine[,1:9]) %*% wi.pca$rotation  # 37(관측치 개수)*9의 행렬이 생성됨

wine.pc<-cbind(as.data.frame(PRC), Aroma)   # 주성분 분석한 가중치(선형 조합) 이용
```



![비교](https://user-images.githubusercontent.com/43332543/59085510-ce5f3d00-8939-11e9-8db3-a16e8bc3b520.PNG)



```R
# 주성분회귀분석
fit1<-lm(Aroma~PC1+PC2+PC3+PC4, data=wine.pc)
summary(fit1)
```



```
Call:
lm(formula = Aroma ~ PC1 + PC2 + PC3 + PC4, data = wine.pc)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.37626 -0.66068  0.00352  0.48748  1.35150 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.7499387  0.8204063   9.446 8.96e-11 ***
PC1          0.0025532  0.0006629   3.852 0.000530 ***
PC2         -0.0147475  0.0046404  -3.178 0.003279 ** 
PC3          0.0031120  0.0082328   0.378 0.707924     
# PC3의 p-value는 유의수준보다 높다(aroma를 예측하는데 별로 중요하지 않다)
# X의 분산을 가장 잘 설명하는 component가 Y를 잘 설명한다는 보장은 없다
PC4         -0.3022774  0.0805886  -3.751 0.000701 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.761 on 32 degrees of freedom
Multiple R-squared:  0.5502,	Adjusted R-squared:  0.494 
# PC1 ~ PC4 다중회귀모형 수행(설명력 0.494)
F-statistic: 9.787 on 4 and 32 DF,  p-value: 2.749e-05
```



```R
fit2<-lm(Aroma~., data=wine.pc)
summary(fit2)
```



```
Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  5.738e+00  7.174e-01   7.998 1.35e-08 ***
PC1          2.553e-03  4.741e-04   5.386 1.08e-05 ***
PC2         -1.475e-02  3.319e-03  -4.444 0.000136 ***
PC3          3.112e-03  5.888e-03   0.529 0.601455    
PC4         -3.023e-01  5.764e-02  -5.244 1.58e-05 ***
PC5         -9.251e-01  2.165e-01  -4.273 0.000214 ***
PC6          1.695e+00  5.323e-01   3.184 0.003639 ** 
PC7         -1.002e+00  1.527e+00  -0.656 0.517085    
PC8         -7.910e+00  3.835e+00  -2.062 0.048902 *  
# 8번째 주성분이면 거의 X분산을 설명하지 못함에도 Y 분산을 설명할 수 있다
PC9         -1.309e+01  8.326e+00  -1.573 0.127409    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.5443 on 27 degrees of freedom
Multiple R-squared:  0.8059,	Adjusted R-squared:  0.7412 
# 우선 설명력은 높아짐
F-statistic: 12.45 on 9 and 27 DF,  p-value: 1.615e-07
```



![잔차 분석](https://user-images.githubusercontent.com/43332543/59085739-97d5f200-893a-11e9-884a-f2203b36bc05.png)





* 부분 최소자승법(PLS)

부분 최소자승회귀법은 공정변수들의 변동을 설명하는 벡터 t를 구하는데 X의 정보만을 이용하는 것이 아니라 **타겟변수 y의 정보**도 동시에 고려한다

![PLS](https://user-images.githubusercontent.com/43332543/59085835-dcfa2400-893a-11e9-8c01-ef116430e43a.PNG)

주성분분석의 component는 독립변수만 고려하므로(독립변수의 분산을 잘 설명하는 것 기준) 반드시 Y를 잘 설명한다고 볼 수 없다

```
PC1을 Y쪽으로 shift해서 X1을 이동시킴
X를 가장 잘 설명하는 weight vector를 산정하고, Y와의 상관성을 고려하는 과정을 iterative하게 실행해 그 차이가 없을 때까지 실시함
```



PLS의 component는 PCR과 다르게 X의 정보를 타겟변수(Y)와의 상관성을 고려하여 도출된다

ex) Chemometrics, Marketing 분야의 고차원데이터, 독립변수간 상관성 높은 데이터에 적용

components는 X들의 벡터의 선형조합으로 산출하는데, 가중치로서 벡터 x를 벡터 y에 projection한 weight를 이용한다



* 실습

```R
library(pls)

data(gasoline)   # pls 패키지에 탑재된 data 사용(데이터 로드)
attach(gasoline)  # 가솔린 데이터(근적외선 흡광도, 60개의 가솔린 표본, 타겟변수(옥탄가), 독립변수 401 차원)

summary(octane)   # 데이터 요약 설명

# 훈련데이터와 검증데이터 구분
gasTrain <- gasoline[1:50, ]   
gasTest <- gasoline[51:60, ]

# 주성분 분석
ga.pca<-prcomp(gasoline$NIR, center=T, scale.=F)
summary(ga.pca)
```



![scree plot](https://user-images.githubusercontent.com/43332543/59087255-19c81a00-893f-11e9-8c4f-446bd1e54833.png)

스크리 그래프를 통해 5(4)개의 PC를 사용하는 것이 좋다



```R
# plsr(타겟변수 ~ 독립변수, ncomp= , data= )
gas1 <- plsr(octane ~ NIR, ncomp = 10, data = gasTrain, validation = "LOO")
# NIR에 401차원(독립변수)의 값이 들어있음
# ncomp = 잠재변수(성분)의 수 = Latent Variable(데이터의 정보가 잠재되어 있는 변수)
# validation = c("none", "CV", "LOO")   # 모형 검증 작업
# cross-validation(교차 검증), leave-one-out(49개로 모델링, 1개로 예측 -> 이것을 50번 반복함)
```



```
Data: 	X dimension: 50 401    # 50개 데이터, 401개 변수
	    Y dimension: 50 1      # 50개 데이터, 1개 변수
Fit method: kernelpls  
Number of components considered: 10   # 잠재변수 수

VALIDATION: RMSEP   # 예측오차(root mean-square error of projection)
Cross-validated using 50 leave-one-out segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps
CV           1.545    1.357   0.2966   0.2524   0.2476   0.2398
adjCV        1.545    1.356   0.2947   0.2521   0.2478   0.2388
       6 comps  7 comps  8 comps  9 comps  10 comps
CV      0.2319   0.2386   0.2316   0.2449    0.2673
adjCV   0.2313   0.2377   0.2308   0.2438    0.2657

# 1개 잠재변수만 사용할 경우, 평균오차가 1.357
# 2개 잠재변수를 사용할 경우, 평균오차가 0.297 ...

TRAINING: % variance explained
        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
X         78.17    85.58    93.41    96.06    96.94    97.89
octane    29.39    96.85    97.89    98.26    98.86    98.96
        7 comps  8 comps  9 comps  10 comps
X         98.38    98.85    99.02     99.19
octane    99.09    99.16    99.28     99.39

# 학습 데이터를 통해 X들의 분산을 얼마나 설명하는가? : 2개의 LV(잠재변수)로 85.58% 설명
# 학습 데이터를 통해 Y값의 변동분 설명비율 : 96.85% 설명

# 이 모델에서는 최소한 2개의 잠재변수를 사용해야 옥탄가를 예측하고 실제 현장에서도 사용할 수 있겠다고 예상
# 우리는 옥탄가만 예측하는 것이 목적이므로 2개 잠재변수만 사용해도 ok
```



* PLS 모형에서의 최적 잠재변수의 수

![최적 잠재변수 개수](https://user-images.githubusercontent.com/43332543/59087734-8394f380-8940-11e9-8a3e-a2d162b669d0.png)

RMSEP가 최소이고 변화가 없는 지점에서 결정 = 2개의 components(LV)를 추천

`예측모형 평가척도 : 평균오차(RMSEP)`



* 최적 PLS 모형의 실제값과 예측값 산점도

```R
plot(gas1, ncomp = 2, asp = 1, line = TRUE, cex=1.5,main="Measured vs Predicted", xlab="Measured" )
# 잠재변수 2개 사용
```



![산점도](https://user-images.githubusercontent.com/43332543/59087821-bfc85400-8940-11e9-92de-e64cfa216a76.png)

```
Measured = 실제 y, predicted = 예측 y
직선은 기울기가 1인 직선을 나타냄 = 직선에서 멀리 떨어지는 점이 없을수록 오차가 작다는 의미
```



* 잠재변수 수에 따른 전체분산의(독립변수들) 설명정도

```
explvar(gas1)

    Comp 1     Comp 2     Comp 3     Comp 4     Comp 5     Comp 6 
78.1707683  7.4122245  7.8241556  2.6577773  0.8768214  0.9466384 
    Comp 7     Comp 8     Comp 9    Comp 10 
 0.4921537  0.4723207  0.1688272  0.1693770 
 
 # 2개의 잠재변수가 분산 전체의 85.58% 설명
```



* 검증 데이터의 RMSEP

```R
ypred<-predict(gas1, ncomp = 2, newdata = gasTest)   # 예측 데이터
y<-gasoline$octane[51:60]   # 실제 데이터

sqrt((sum(y-ypred)^2)/10)
```



```
RMSEP(gas1, newdata = gasTest)   # 잠재변수 개수에 따라 달라지는 RMSEP

(Intercept)      1 comps      2 comps      3 comps      4 comps  
     1.5369       1.1696       0.2445       0.2341       0.3287  
    5 comps      6 comps      7 comps      8 comps      9 comps  
     0.2780       0.2703       0.3301       0.3571       0.4090  
   10 comps  
     0.6116 
```

