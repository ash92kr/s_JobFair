### 16강 딥러닝과 텍스트마이닝



* 인공신경망(Neural Network)

인공신경망은 기계학습의 통계적 학습 알고리즘 중 하나로 컴퓨터 비젼, 자연어 처리, 음성 인식 등에서 활발하게 사용됨

![개념 범위](https://user-images.githubusercontent.com/43332543/59172921-7fa5e300-8b85-11e9-8bbb-19816de26985.PNG)

신경망 모델은 퍼셉트론을 한 단위로 하는 네트워크를 구축해, 인간의 신경세포와 유사한 기능을 하도록 제안되었음

input(독립변수) -> hidden layer(가중치) -> output(종속변수)



* 퍼셉트론(단일 계층)

하나의 퍼셉트론은 단순하게 다수의 입력과 가중치의 선형 결합을 계산하는 역할 수행

활성함수에 따라 선형결합으로 생성되는 출력값이 결정됨

![활성함수](https://user-images.githubusercontent.com/43332543/59173092-35713180-8b86-11e9-8e83-6fab59940241.PNG)

최근에는 Sigmoid보다 ReLU를 쓰는 경우가 더 많다



* 다층 퍼셉트론

퍼셉트론으로 구성된 Single layer들이 다층을 만듦

입력층과 출력층 사이에 Hidden Layer가 존재해 Non-linear transformation을 수행한다

출력층에서 Softmax 함수를 통해 가장 큰 값을 손쉽게 알 수 있다(exponential 함수를 통해 항상 양수값이 도출되고 이를 통해 확률값을 도출함)

![다층 퍼셉트론](https://user-images.githubusercontent.com/43332543/59173200-c5af7680-8b86-11e9-9e8a-2887c5c43914.PNG)



* 실습

```R
install.packages("https://github.com/jeremiedb/mxnet_winbin/raw/master/mxnet.zip",repos = NULL)    # mxnet 패키지 필수
library(mxnet)

iris<-read.csv("iris.csv")

iris[,5] = as.numeric(iris[,5])-1    # Species를 label로 활용(각 종별로 0 ~ 2의 숫자로 변화함)
```



```R
# 데이터 분할
set.seed(1000)
N<-nrow(iris)
tr.idx<-sample(1:N, size=N*2/3, replace=FALSE)

train<-data.matrix(iris[tr.idx,])
test<-data.matrix(iris[-tr.idx,])

# 각 데이터별 feature와 label로 분리함
train_feature<-train[,-5]
trainLabels<-train[,5]
test_feature<-test[,-5]
testLabels <-test[,5]
```



```R
# hidden laber 구성
my_input = mx.symbol.Variable('data')

fc1 = mx.symbol.FullyConnected(data=my_input, num.hidden = 200, name='fc1')
# 200개의 뉴런을 형성해 4개의 input과 모두 연결함(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)
relu1 = mx.symbol.Activation(data=fc1, act.type='relu', name='relu1')
# ReLU Function 사용

# 두 번째 hidden layer는 100개를 만들고, 앞선 200개 hidden layer와 모두 연결함
fc2 = mx.symbol.FullyConnected(data=relu1, num.hidden = 100, name='fc2')
relu2 = mx.symbol.Activation(data=fc2, act.type='relu', name='relu2')
```



```R
# Output Layer 구성(3개로 분류해야 하므로 3개의 output 생성)
fc3 = mx.symbol.FullyConnected(data=relu2, num.hidden = 3, name='fc3')

# softmax 결과를 통해 가장 큰 값을 선택한다
softmax = mx.symbol.SoftmaxOutput(data=fc3, name='sm')
```



```R
# 앞서 만든 Layer를 이용해 모델 형성 및 학습
mx.set.seed(123)
device <- mx.cpu()
model <- mx.model.FeedForward.create(softmax,
                                     optimizer = "sgd",
                                     array.batch.size=25,
                                     num.round = 500, learning.rate=0.1,
                                     X=train_feature, y=trainLabels, ctx=device,
                                     eval.metric = mx.metric.accuracy,
                                     array.layout = "rowmajor", 			                                 epoch.end.callback=mx.callback.log.train.metric(100))

# 옵티마이저(Stochastic Gradient Descent)
# 배치 사이즈(그룹별로 딥러닝 수행) : 전체 데이터를 10 그룹으로 나눔
# 반복(iteration, epoch) 500번
# 학습률(Learning Step) 0.1
```



```R
# 검증 데이터로 검증
predict_probs <- predict(model, test_feature, array.layout = "rowmajor")
# NN 모델, test data set의 X데이터 넣음
predicted_labels <- max.col(t(predict_probs)) - 1    # 모델을 통한 예측값
table(testLabels, predicted_labels)
sum(diag(table(testLabels, predicted_labels)))/length(predicted_labels)
# test data set의 정확도가 나옴
```



```
          predicted_labels
testLabels  0  1  2
         0 16  0  0
         1  0 15  0
         2  0  2 17
```



![전체 plot](https://user-images.githubusercontent.com/43332543/59173648-d8c34600-8b88-11e9-9d6a-57161d622f5c.png)



* CNN(심층 신경망)

신경망 모델은 입력값으로 객체의 특성(feature)을 받고, 출력값과 실제값을 비교하는 과정을 거침(지도학습)

하나의 이미지는 수많은 픽셀들이 모여 형성하며, 특정 색에 해당하는 특정 값을 가짐 → 이미지의 모든 픽셀값들을 입력값으로 갖는 신경망 모델을 만들 수 있음



그러나 고해상도 이미지는 특성(feature)의 수가 많아지므로 모든 뉴런 * 모든 픽셀이 연결된 경우(fully connected) 모델 학습에 큰 어려움이 있음

따라서, 각 뉴런들이 이미지의 일부 특성들과만 연결될 수 있는 구조가 더 적합함 = Convolution operation을 통해 구현 가능

![CNN](https://user-images.githubusercontent.com/43332543/59173779-7454b680-8b89-11e9-8889-fa24cfa4e054.PNG)

CNN = 일부의 feature와 연결하는 나선형 구조



* Convolution Operation

임의의 값으로 설정된 filter가 전체 이미지 중 일부의 선형 결합을 계산함

각각의 결과값은 하나의 Neuron(뉴런)이 되며, filter는 해당 뉴런의 가중치가 됨

결과값의 사이즈를 정하기 위해서는 Stride, Padding, Depth를 고려해야 함

![Convolution](https://user-images.githubusercontent.com/43332543/59173878-e6c59680-8b89-11e9-8f93-2e48cf42c4c1.PNG)

filter = Locally Connected된 가중치

Depth = number of filter(뉴런의 개수)

Stride = filter를 몇 칸 이동할지 결정

Padding = 이미지 주변으로 0으로 둘러싼 범위를 넣음

Output = image와 filter를 거쳐서 나온 숫자들을 모두 더한 값



* Pooling : Convolutional layer 사이에 pooling layer를 넣어주는 방법

추출해낸 이미지에서 지역적인 부분의 특징만 뽑아 다음 layer로 넘겨줌 → 가중치 수를 줄일 수 있으며, 과적합을 방지함

대표적으로 가장 큰 값 만을 뽑아내는 Max pooling이 많이 사용됨

![Pooling](https://user-images.githubusercontent.com/43332543/59174075-b7635980-8b8a-11e9-9ee8-4d75bdc3d6d6.PNG)



* 실습(MNIST 데이터 - 손으로 쓴 숫자들을 인식하기 위해 사용되는 데이터)



```
library(mxnet)

mn1 <- read.csv("mini_mnist.csv")
set.seed(123)
N<-nrow(mn1)
tr.idx<-sample(1:N, size=N*2/3, replace=FALSE)   # 학습 데이터 2/3, 테스트 데이터 1/3

train_data<-data.matrix(mn1[tr.idx,])
test_data<-data.matrix(mn1[-tr.idx,])

# 정규화(0과 1사이에 분포하도록, 0 = 검정색, 255 = 흰색)
test<-t(test_data[,-1]/255)
features<-t(train_data[,-1]/255)
labels<-train_data[,1]
```



```
# 입력 데이터의 차원을 설정(픽셀 * 객체 개수)
# ncol(features) : 학습 데이터 수(866)
features_array <- features
dim(features_array) <- c(28, 28, 1, ncol(features))   # 이미지를 28*28*1로 나눔
test_array <- test
dim(test_array) <- c(28, 28, 1, ncol(test))

ncol(features)
table(labels)
```



```
# Convolutional Layer 구성
my_input = mx.symbol.Variable('data')
conv1 = mx.symbol.Convolution(data=my_input, kernel=c(4,4), stride=c(2,2), pad=c(1,1), num.filter = 20, name='conv1')
# 이미지의 크기 : padding을 1씩 넣어 상/하/좌/우 30이 됨
# 필터의 크기 : kernel의 크기(4*4)
# 결과 이미지 크기 : 14*14*20(stride가 2이므로 2칸씩 건너뜀, 필터 개수가 20)
relu1 = mx.symbol.Activation(data=conv1, act.type='relu', name='relu1')
mp1 = mx.symbol.Pooling(data=relu1, kernel=c(2,2), stride=c(2,2), pool.type='max', name='pool1')

# second conv layers
conv2 = mx.symbol.Convolution(data=mp1, kernel=c(3,3), stride=c(2,2), pad=c(1,1), num.filter = 40, name='conv2')
# 이미지 크기 : 14*14*20
# 필터 크기 : 3*3
# 결과 이미지 크기 : 4*4*40(필터 개수 40, stride = 2칸씩 건너뜀)
relu2 = mx.symbol.Activation(data=conv2, act.type='relu', name='relu2')
mp2 = mx.symbol.Pooling(data=relu2, kernel=c(2,2), stride=c(2,2), pool.type='max', name='pool2')

# fully connected
fc1 = mx.symbol.FullyConnected(data=mp2, num.hidden = 1000, name='fc1')
# 1000개 뉴런들이 모두 연결되어 있음
relu3 = mx.symbol.Activation(data=fc1, act.type='relu', name='relu3')
fc2 = mx.symbol.FullyConnected(data=relu3, num.hidden = 3, name='fc2')

# softmax - 3개 뉴런들 중 가장 확률이 높은 값이 0~2중 하나를 가리킴
sm = mx.symbol.SoftmaxOutput(data=fc2, name='sm')
```



![mnist](https://user-images.githubusercontent.com/43332543/59174753-a49e5400-8b8d-11e9-9ee8-eb76c6c9f5a0.PNG)



```
# 모델 훈련
mx.set.seed(100)
device <- mx.cpu()
model <- mx.model.FeedForward.create(symbol=sm, 
                                     optimizer = "sgd",
                                     array.batch.size=30,
                                     num.round = 70, learning.rate=0.1,
                                     X=features_array, y=labels, ctx=device,
                                     eval.metric = mx.metric.accuracy,                                       epoch.end.callback=mx.callback.log.train.metric(100))
# 옵티마이저 : Stochastic Gradient Descent
# 배치 사이즈 : 30(29개 그룹)
# 반복 횟수(epoch) : 70
# 학습률(Learning Step) : 0.1
```



```
# 검증 데이터를 통한 검증률
predict_probs <- predict(model, test_array)
predicted_labels <- max.col(t(predict_probs)) - 1
table(test_data[, 1], predicted_labels)
sum(diag(table(test_data[, 1], predicted_labels)))/length(predicted_labels)
```



```
   predicted_labels
      0   1   2
  0 139   0   1
  1   2 145   5
  2   3   0 138
```



![mnist plot](https://user-images.githubusercontent.com/43332543/59174961-7e2ce880-8b8e-11e9-9a38-709f7fa7005e.png)



* 텍스트마이닝(한글 웹문서의 자연어 처리와 정보 추출)

텍스트 마이닝이란, 다양한 알고리즘을 이용해 대용량의 텍스트 문서로부터 트랜드와 관심어를 찾아내는 기법이다

ex) 키워드 분석 : 단어 **출현 빈도** 추출 후 시각화

단어간 관계 분석 : 문서 내 출현 빈도 높은 단어 파악 후 **단어간 상관관계** 계산

감성 분석 : 긍정 단어와 부정 단어 빈도 추출 후 **문서 감성** 수치화



* 자연어처리 : 컴퓨터로 사람 언어를 분석, 이해, 생성하는 기술

ex) 영어 자연어 처리 패키지(NLP), 한국어 자연어 처리 패키지(KoNLP)



* 한글 품사 분석

SimplePos09() - N(체언), P(용언), E(어미), J(관계언), S(기호), F(외국어)

str_match()를 이용한 N(체언), P(용언) 추출

![simplepos09](https://user-images.githubusercontent.com/43332543/59175169-791c6900-8b8f-11e9-8a64-096badebbf2e.PNG)



* 워드 클라우드 : 텍스트의 키워드, 개념을 직관적으로 파악하도록 핵심 단어를 시각적으로 보여주는 기법



* 실습

(1) 네이버 영화에서 영화 '머니볼'의 네티즌 140자 평을 크롤링함

(2) 단어 문서 행렬 산출

(3) 출현 빈도가 높은 단어들의 워드 클라우드를 생성

(4) 단어 사이의 상관관계를 보여주는 동시 발생 행렬 산출

(5) 동시 발생 행렬을 바탕으로 단어들의 네트워크를 생성



```
library(xml2)   # html 처리
library(rvest)   # html 처리
library(KoNLP)   # 국문 자연어 처리
library(tm)   # 정보 추출 도구
library(stringr)   # string 처리
library(wordcloud)   # 워드 클라우드
libary(qgraph)   # 네트우크
```



* 한글 인코딩 방식 변경 : 상단 메뉴바 Tools - Global options - Code - Saving -> UTF-8로 변경



* 크롤링 : 웹페이지를 그대로 가져와서 데이터를 추출해내는 행위, 데이터를 개인 하드에 소장하는 것은 합법이지만, 배포는 불법



<https://movie.naver.com/movie/bi/mi/basic.nhn?code=51786>

페이지 소스 보기(Ctrl + U)

```
# 크롤링 대상 URL
url_base <- 'http://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=51786&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page='

# 140자 평 저장 벡터
reviews <- c()

# 1부터 10페이지까지 데이터 수집
for(page in 1:10){
  url <- paste(url_base, page, sep='') # from page 1 to 70
  htxt <- read_html(url)
  comment <- html_nodes(htxt, 'div')%>%html_nodes('div.input_netizen')%>%
    html_nodes('div.score_result')%>%html_nodes('ul')%>%html_nodes('li')%>%
    html_nodes('div.score_reple')%>%html_nodes('p')
    # exact location of comments(140자 평 위치)
  review <- html_text(comment) 
  # extract only texts from comments(실제 리뷰의 text 파일만 추출)
  review <- repair_encoding(review, from = 'utf-8')
  # repair faulty encoding(인코딩 변경)
  review <- str_trim(review)
  # trim whitespace from start and end of string(앞뒤 공백 문자 제거)
  reviews <- c(reviews, review)
  # save results(결과값 저장)
}
```



```
# 용언, 체언 추출
ext_func <- function(doc){   # 인자로 받는 문서
  doc_char <- as.character(doc)
  ext1 <- paste(SimplePos09(doc_char))   # 품사 분석
  ext2 <- str_match(ext1, '([A-Z가-힣]+)/[NP]')   # 용언, 체언 선택
  keyword <- ext2[,2]
  keyword[!is.na(keyword)]   # NA값 제외
}
```



```
# 단어-문서 행렬
corp <- Corpus(VectorSource(reviews))  # generate a corpus(말뭉치 생성)

# generate a term-document matrix(단어-문서 행렬 산출)
tdm <- TermDocumentMatrix(corp,
                          control=list(
                            tokenize=ext_func,
                            removePunctuation=T,
                            removeNumbers=T,
                            wordLengths=c(4,8)))

tdm_matrix <- as.matrix(tdm)  # save as a matrix(행렬 변환)
Encoding(rownames(tdm_matrix)) <- "UTF-8"  # encoding(인코딩)

word_count <- rowSums(tdm_matrix)
# sum of term frequencies of each word(각 단어별 총 출현 빈도 계산)
word_order <- word_count[order(word_count, decreasing=T)]
# sort in descending order(내림차순 정렬)

doc <- as.data.frame(word_order) # save as a data frame(데이터 프레임으로 변환)
```



```
# 워드 클라우드
library(wordcloud)

windowsFonts(font=windowsFont("맑은 고딕")) # set font
set.seed(1234)   # 동일한 워드 클라우드 생성(난수 고정)

wordcloud(words=rownames(doc), freq=doc$word_order, min.freq=2,
          max.words=100, random.order=FALSE, scale=c(5,1), rot.per=0.35,
          family="font", colors=brewer.pal(8,"Dark2"))
          # 키워드, 빈도, 최소 출현 빈도, 출력 키워드 수, 고빈도 키워드 중앙 배치, 키워드 크기 범위, 회전 키워드 비율
```

![워드 클라우드](https://user-images.githubusercontent.com/43332543/59175857-0496f980-8b92-11e9-88d3-e75fad9e735b.png)

* 단어간 관계분석(네트워크 그래프), 감성분석(막대 그래프)