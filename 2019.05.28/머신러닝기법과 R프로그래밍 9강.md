# 머신러닝기법과 R프로그래밍



### 9강 데이터 마이닝 기초



(1) 다중회귀분석1



* 데이터마이닝 기법

예측 : 종속변수(y)의 값이 연속적인 경우 ex) 다중회귀분석, 주성분분석, 신경망

분류 : 종속변수의 값이 범주형인 경우, 분류규칙을 만들고 실제범주와 추정된 범주 비교  ex) 로지스틱 회귀분석, 의사결정나무, SVM 등



군집/연관분석 : 타겟변수 값이 없는 경우(x변수만 존재) ex) 계층형 군집분석, k-means, 연관규칙



데이터 마이닝 > 지도 학습 + 비지도 학습 > 통계 분석, 머신 러닝(지도 학습 내 존재)



* 다중회귀모형

종속변수 Y를 설명하는데 k개의 독립변수가 있을 때의 다중회귀모형

![다중회귀분석 모형](https://user-images.githubusercontent.com/43332543/58453489-58e3b780-8156-11e9-8403-678adaaf74cd.PNG)

회귀계수의 Bk의 해석 : 다른 독립변수들이 일정할 때 X k의 1단위 변화에 따른 평균변화량



autompg 데이터 실습



```R
Call:  # 전체변수를 모두 포함한 회귀모형
lm(formula = mpg ~ disp + hp + wt + accler, data = car)  

Residuals:  # 잔차
     Min       1Q   Median       3Q      Max 
-11.8331  -2.8735  -0.3164   2.4449  16.2079 

Coefficients:  # 회귀계수  # 표준오차      # p-통계량(0.05 이하여야 유의미)
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) 40.8838025  1.9966258  20.476  < 2e-16 ***
disp        -0.0106291  0.0065254  -1.629   0.1041 
hp           0.0047774  0.0082597   0.578   0.5633  # 마력이 좋을수록 연비 좋음???
wt          -0.0061405  0.0007449  -8.243 2.54e-15 ***
accler       0.1722165  0.0976340   1.764   0.0785 .  
---   # mpg = 40.88 - 0.011disp + 0.0048hp - 0.0061wt + 0.17accler
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 4.298 on 393 degrees of freedom   # 표준오차
Multiple R-squared: 0.7006, Adjusted R-squared:  0.6976
# 결정계수(설명력) -> y의 변동분을 x가 얼마나 잘 설명하는가?
F-statistic: 230 on 4 and 393 DF, p-value: < 2.2e-16  # F통계량(모형 전체 적합도)
```





![변수 상관관계표](https://user-images.githubusercontent.com/43332543/58453277-77957e80-8155-11e9-8acb-2f6370f31f11.png)

```
가장 윗 줄의 mpg와 독립변수 사이의 산점도 확인
오른쪽 위의 독립변수들 사이의 산점도 확인
```





(2) 다중회귀분석2



* 변수선택방법

전진선택법 : 독립변수 중 종속변수에 가장 큰 영향을 주는 변수부터 모형에 포함

후진제거법 : 독립변수를 모두 포함한 모형에서 가장 영향이 적은(중요하지 않은) 변수부터 제거

단계별방법 : 전진선택법에 의해 변수 추가 + 추가시 기존 변수의 중요도가 정해진 유의수준(threshold)에 포함되지 않으면 앞에서 들어간 변수도 다시 제거됨



```R
Start:  AIC=1165.67  # 단계별 선택방법에 의한 회귀모형
mpg ~ disp + hp + wt + accler

         Df Sum of Sq    RSS    AIC  
- hp      1      6.18 7266.2 1164.0  # 이 변수를 제거한다
<none>                7260.0 1165.7
- disp    1     49.01 7309.1 1166.3
- accler  1     57.48 7317.5 1166.8
- wt      1   1255.16 8515.2 1227.1

Step:  AIC=1164.01   # AIC가 가장 낮은 조합의 변수그룹 선택 = 결정계수가 가장 높은 조합
mpg ~ disp + wt + accler

         Df Sum of Sq    RSS    AIC
<none>                7266.2 1164.0   # none이 빠지면 끝
- disp    1     51.76 7318.0 1164.8
- accler  1     58.62 7324.8 1165.2
+ hp      1      6.18 7260.0 1165.7
- wt      1   1291.30 8557.5 1227.1

Call:
lm(formula = mpg ~ disp + wt + accler, data = car)

Coefficients:
(Intercept)         disp           wt       accler  
  41.299076    -0.010895    -0.006189     0.173851 
```



* 회귀분석의 가정과 진단 = 오차의 산점도



![오차 산점도](https://user-images.githubusercontent.com/43332543/58453216-31401f80-8155-11e9-92da-cfd94f6676f6.png)



```
(1) Residuals vs Fitted : 예측값과 잔차 사이의 산점도
(2) Normal Q-Q : 오차가 정규분포를 따르는가?
```



* 다중공선성 : 독립변수들 사이에 상관관계가 매우 높은 경우 발생함



다중공선성이 있는 경우 회귀분석 결과를 신뢰할 수 없어 회귀계수 해석이 어려움

회귀계수 부호가 바뀌거나 회귀모형의 안정성이 떨어짐

```R
> cor(car[var2])   # 변수 간 상관계수 확인
             disp         hp         wt     accler
disp    1.0000000 -0.4785123  0.9328241 -0.5436841   # disp와 wt의 상관계수가 높다
hp     -0.4785123  1.0000000 -0.4807430  0.2566567
wt      0.9328241 -0.4807430  1.0000000 -0.4174573
accler -0.5436841  0.2566567 -0.4174573  1.0000000
```



상관계수가 높다고 해서 반드시 다중공선성이 생기는 것은 아니다 -> 다중공선성이 생길 경우 적절하게 처리해야 함



* VIF(분산팽창계수) : 다중공선성의 척도

VIF는 다중공선성으로 인한 분산의 증가를 의미한다 -> 10 이상이면 다중공선성을 고려한다

![VIF](https://user-images.githubusercontent.com/43332543/58453487-584b2100-8156-11e9-8ed4-6260463a5f51.PNG)

Rj2는 Xj를 종속변수로 하고 나머지 변수를 독립변수로 하는 회귀모형에서의 결정계수이다



* 다중공선성이 생기는 경우 처리 방법

```
상관계수가 높은 두 변수 중 하나만 선택
더 많은 데이터 수집
능형회귀/주성분회귀 분석법 사용
```





* 회귀모형 결정하기

```
(1) 회귀계수와 결정계수
(2) 다중공선성(vif)
(3) 잔차도
(4) 이상치나 특이 패턴
```



* 탐색적 분석

![탐색적 분석](https://user-images.githubusercontent.com/43332543/58453697-140c5080-8157-11e9-84eb-06add068f079.png)



```
마력이 높을수록 연비가 낮다
-> 실제로는 두 개의 클러스터로 구분됨을 확인
```



하나의 변수를 기준선을 정해 하위 subset을 생성한다

```R
car_s1<-subset(car, hp<50)
plot(car_s1$hp, car_s1$mpg,col=10,  main="hp<50")
summary(lm(car_s1$mpg ~ car_s1$hp))  # 회귀식과 결정계수 확인

car_s2<-subset(car, hp>=50)
plot(car_s2$hp, car_s2$mpg, col="coral", main="hp>=50")
summary(lm(car_s2$mpg ~ car_s2$hp))
```





(3) 분류규칙과 과적합



* 분류분석 : 다수의 속성을 갖는 객체를 그룹/범주로 분류하기

학습표본으로부터 효율적인 분류규칙(오분류율/cost function을 최소화) 생성하기

→ 새로운 데이터로 예측/분류하기



* 오분류율 = 오분류 객체수(케이스) / 전체 객체수(케이스)



* 과적합 : 학습표본에 대해 오분류율을 0으로 인위적으로 만드는 경우

```
분류모형에서 훈련데이터에 대한 과적합을 시킬 경우, 실제 데이터를 적용했을 때 더 높은 오분류율이 발생한다

과적합 문제를 방지하기 위해 학습데이터와 검증데이터를 일반적으로 7:3으로 분리해 모형의 성능을 비교 평가한다
```



* 교차검증(cross-validation)

```
데이터 수집 -> 학습데이터/검증데이터 분리 -> 학습데이터로 분류규칙 생성 -> 분류규칙에 검증데이터 투입 -> 결과 확인(양쪽 데이터의 오분류율이 비슷해야 현장에서 활용 가능)
```



k-fold cross validation method : 데이터를 k등분으로 나누어 k-1등분은 학습데이터로 분류규칙을 생성하고, 나머지 k번째 데이터로 검증하는 방식(데이터는 랜덤하게 나누어야 한다)



(4) 학습데이터와 검증데이터



* iris 데이터 - 꽃잎의 폭과 길이에 대한 4개 변수 -> 꽃의 종류 예측



* 학습데이터와 검증데이터 생성

```R
set.seed(1000)
# seed 번호를 주어야 난수 생성 시 동일한 표본 대상으로 사용 가능함
# 넣지 않으면 매번 다른 훈련표본 생성성
N=nrow(iris)   
tr.idx=sample(1:N, size=N*2/3, replace=FALSE)   # 3-fold cross-validation
tr.idx
```



```R
# attributes in training and test
iris.train<-iris[tr.idx,-5]  # 종속변수 제외한 독립변수 데이터
iris.test<-iris[-tr.idx,-5]  # 위 100개의 행을 제외한 50개 데이터
# target value in training and test
trainLabels<-iris[tr.idx,5]  # 종속변수 데이터터
testLabels<-iris[-tr.idx,5]
```

